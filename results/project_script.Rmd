---
title: "R Notebook"
#output: html_notebook
---

# Loading data
## Load data into R
```{r}
# set root directory
root = "/home/student12/project/"

# Load the metadata, use paste0() to concatenate two strings
sample_metadata <- read_tsv(paste0(root,"data/sample_metadata.txt"))

# inspect metadata
head(sample_metadata)
```

# QC Preprocessing
Trim to minimal 35 bases and 35 quality score: **Trimmomatic v0.22**

## Packages and software
The trimming and quality control is done by using the software Trimmomatic. The version in the article is v0.22, we will operate on the current release v0.39. Although not a part of the original study, we will download the fastQC software for simple data visualization before and after trimming.

```{bash}
# Initiate conda
source ~/.bashrc

# Check current packages
conda list

# activate conda environment
conda activate project

# The project environment is a clone of the sequencing conda environment containing:
# conda install samtools bowtie2 breseq abyss bcftools wgsim emboss tree

conda install -c bioconda trimmomatic

# Not mandatory: install fastqc to view fastq files in browser
conda install -c bioconda fastqc

```

Run fastQC software to inspect data. This will generate a .fastqc.zip file and a .html file which can be opened in a browser through Rstudio. This will show basic statistics for the read, including its average quality score across reads.

```{bash}
fastqc test_data/TEST_FILE
```


## Trimming and quality control
Trimming and quality control may now be performed using Trimmomatic v0.39. 

```{bash}
mkdir QC_test_data

trimmomatic PE -threads 4 test_ERR4013290_1.fastq test_ERR4013290_2.fastq  \
              QC_test_data/test_ERR4013290_1.trimmed.fastq                         QC_test_data/test_ERR4013290_1un.trimmed.fastq \
              QC_test_data/test_ERR4013290_2.trimmed.fastq QC_test_data/test_ERR4013290_2un.trimmed.fastq \
              TRAILING:35 MINLEN:35

```

# de novo assembly
**ABySS v1.3.7**
ABySS v1.3.7 with k values from 20 to 40 was used for de novo assembly of pair-end reads and de novo consensus sequences generated.

```{bash}
# Install ABySS using conda
conda install -c bioconda abyss
```

Run ABySS
```{bash}
mkdir abyss_pe

abyss-pe name=WTZIKA k=20 in="test_ERR4013290_1.trimmed.fastq 
                              test_ERR4013290_2.trimmed.fastq" 
                              
# Move files to folder
mv WTZIKA* abyss_pe
mv coverage* abyss_pe

mkdir abyss_pe_se

abyss-pe name=WTZIKA k=20 pe="test_ERR4013290_1.trimmed.fastq test_ERR4013290_2.trimmed.fastq"                              se="test_ERR4013290_1un.trimmed.fastq test_ERR4013290_2un.trimmed.fastq" 
mv WTZIKA* abyss_pe_se
mv coverage* abyss_pe_se
```

Choosing to skip the de novo sequences generated by ABySS due to strange output. Moving directly from here to Alignment with reference genome instead.

# SKIPPED: Disassembly BAM files
**SamToFastq.jar** and **SAMtools**


# Alignment
(reference, de novo concensus) with very sensitive local alignment:
**Bowtie2 v2.2.4**

A full genome for WT ZIKV FSS13025 was downloaded from GenBank (https://www.ncbi.nlm.nih.gov/nuccore/MH158236.1) to be used as reference sequence for alignment. This was done instead of using the failed de novo sequence generated by ABySS, as per the study. The deviations should in theory be minor but point mutations may well be present in already in the P0 WT ZIKV sequenced in the study.

For alignment Bowtie2 was used with specified local alignment mode and the very sensitive default setting consisting of Qphred of 20, max number of mismatch of 0, length of seed substring 20, and interval between seed substrings S,1,0.5 to apply a stringent alignment. 

```{bash}
conda deactivate

# Activate sequencing environment
conda activate sequencing

# Make new directoies to facilitate bowtie2 output
mkdir test_data/QC_test_data/bowtie_index
mkdir test_data/QC_test_data/alignment

# Build index for reference genome
bowtie2-build -f WTZIKV_FSS13025_reference.fasta test_data/QC_test_data/bowtie_index/WTZIKV
```


```{bash}
# Run bowtie2 on both paired trimmed reads (-1 and -2) and on unpaired trimmed reads (-U)
bowtie2 --very-sensitive-local -x test_data/QC_test_data/bowtie_index/WTZIKV -1 test_data/QC_test_data/test_ERR4013290_1.trimmed.fastq -2 test_data/QC_test_data/test_ERR4013290_2.trimmed.fastq -U test_data/QC_test_data/test_ERR4013290_1un.trimmed.fastq, test_data/QC_test_data/test_ERR4013290_2un.trimmed.fastq -S test_data/QC_test_data/alignment/WTZKVIP0.sam 
```


# Compress SAM file and sort by coordinate
**Samtool view** and **SortSam** using:
**SAMtools v1.1** and **Picard-tools v1.128**

Convert alignment .sam file to binary .bam format.
```{bash}
conda activate sequencing

# Convert using samtools view
samtools view -S -b test_data/QC_test_data/alignment/WTZKVIP0.sam > test_data/QC_test_data/alignment/WTZKVIP0.bam

```

```{bash}

# Install Picard
conda install -c bioconda picard

```

```{bash}

# Run picard on .bam file
picard SortSam \
      I=test_data/QC_test_data/alignment/WTZKVIP0.bam \
      O=test_data/QC_test_data/alignment/WTZKVIP0_sorted.bam \
      SORT_ORDER=coordinate

# View the sorted .bam file
samtools view test_data/QC_test_data/alignment/WTZKVIP0_sorted.bam | less -S
```


# Remove PCR duplicates
**MarkDuplicates** and **Picard-tools v1.128**

Run picard MarkDuplicates on the sorted .bam file to remove PCR-duplicates. 

Left to do: Check the files before and after removal, what has changed? 

```{bash}
# Remove duplicates
picard MarkDuplicates \
      I=test_data/QC_test_data/alignment/WTZKVIP0_sorted.bam \
      O=test_data/QC_test_data/alignment/WTZKVIP0_sorted_removed_duplicates.bam \
      M=test_data/QC_test_data/alignment/WTZKVIP0_sorted_removed_dup_metrics.txt \
      REMOVE_DUPLICATES=true

# View the data
samtools view test_data/QC_test_data/alignment/WTZKVIP0_sorted_removed_duplicates.bam | less -S
```


# Variant detection
**V-phaser2 v2.0**

```{bash}
# Make a new directory for storing the Vphaser output
mkdir variant_calling

conda activate sequencing

# Install Vphaser2
conda install -c bioconda vphaser2

```


```{bash}

# Run Vphaser2 on the duplicate removed, sorted file
vphaser2 test_data/QC_test_data/alignment/WTZKVIP0_sorted_removed_duplicates.bam \
          variant_calling

```


# Shannon entropy
**RF** and **bam2R** using:
**deepSNV v1.16.0**

```{r}
new_entropy = function(x){
  x[1:5]
  new_entropy = -sum(x*log(x), na.rm = TRUE)
  
  return(new_entropy)}
```


